{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539367ee-57e4-4672-ac66-2b4b75c379b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "model = hub.load('https://tfhub.dev/google/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7840454f-dabf-4da4-a3ac-36062d2a0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 206\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "taxonomy = pd.read_csv(\"/Users/rohitbogulla/Desktop/Sem 2/ML/Project/BirdClef/birdclef-2025/taxonomy.csv\")\n",
    "\n",
    "label_list = taxonomy['primary_label'].tolist()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(label_list)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "print(\"Total classes:\", len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573d2b7e-665b-4a42-956e-69ceee8c87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the input ogg file to a frequency array \n",
    "# length of frequency array is (sample_rate(sr) / hop_length) * length_of_audio_in_seconds\n",
    "\n",
    "def ogg_to_frequency_array(ogg_file_path, frame_length=1024, hop_length=256):\n",
    "    \"\"\"\n",
    "    Converts OGG audio data to an array of frequencies.\n",
    "\n",
    "    Args:\n",
    "        ogg_file_path (str): Path to the OGG audio file.\n",
    "        frame_length (int): Length of the FFT window.\n",
    "        hop_length (int): Step size between successive FFT windows.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of frequencies over time.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(ogg_file_path, sr=44100)\n",
    "    frequencies = librosa.fft_frequencies(sr=sr, n_fft=frame_length)\n",
    "    stft_result = np.abs(librosa.stft(y, n_fft=frame_length, hop_length=hop_length))\n",
    "    dominant_frequencies = np.argmax(stft_result, axis=0)\n",
    "    frequency_array = frequencies[dominant_frequencies]\n",
    "\n",
    "    return frequency_array\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     file_path = '/Users/rohitbogulla/Desktop/Sem 2/ML/Project/BirdClef/birdclef-2025/train_audio/21038/iNat297879.ogg'\n",
    "#     frequency_data = ogg_to_frequency_array(file_path, 1024, 256)\n",
    "#     print(len(frequency_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8bbfff-1943-468d-a089-0ca1a1d2c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting audio to MFCC (Mel-Frequency Cepstral Coefficients) \n",
    "# data of the form [n_mfcc, num_frames]\n",
    "\n",
    "def fixed_length_mfcc(audio_path, sr=44100, duration=10, n_mfcc=10):\n",
    "    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "    \n",
    "    # Pad/truncate\n",
    "    target_len = sr * duration\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684654c6-e0f2-4e66-b35a-e0e2e489a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yamnet_embedding(audio_path, sr=44100):\n",
    "    # Load and resample audio\n",
    "    y, _ = librosa.load(audio_path, sr=sr)\n",
    "    waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    # Run through YAMNet\n",
    "    scores, embeddings, spectrogram = model(waveform)\n",
    "    embeddings_np = embeddings.numpy()\n",
    "\n",
    "    # Return mean embedding over time (fixed-size vector)\n",
    "    return np.mean(embeddings_np, axis=0)  # shape: (1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0647cad0-08e7-42ce-9b2a-5ab3e3b31b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_logmelspec(path, random_crop=False):\n",
    "    sr = 32000\n",
    "    n_mels = 128\n",
    "    y, _ = librosa.load(path, sr=sr, mono=True)\n",
    "\n",
    "    if random_crop:\n",
    "        segment_samples = int(sr * 5.0)\n",
    "        if len(y) > segment_samples:\n",
    "            max_start = len(y) - segment_samples\n",
    "            start = np.random.randint(0, max_start)\n",
    "            y = y[start:start + segment_samples]\n",
    "        else:\n",
    "            y = np.pad(y, (0, max(0, segment_samples - len(y))))\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=2048,\n",
    "        hop_length=512,\n",
    "        n_mels=n_mels,\n",
    "        fmin=20,\n",
    "        fmax=16000\n",
    "    )\n",
    "    logmel = librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
    "    logmel_norm = (logmel - logmel.min()) / (logmel.max() - logmel.min())\n",
    "\n",
    "    melspec = np.expand_dims(logmel_norm, axis=0)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fde36034-1f1a-4461-a283-001c5cfa8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(self, df, label_to_idx, audio_root, sr=32000, n_mels=128, target_len=309, random_crop=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.audio_root = audio_root\n",
    "        self.sr = sr\n",
    "        self.n_mels = n_mels\n",
    "        self.target_len = target_len\n",
    "        self.random_crop = random_crop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def audio_to_logmelspec(self, path):\n",
    "        y, _ = librosa.load(path, sr=self.sr, mono=True)\n",
    "\n",
    "        if self.random_crop:\n",
    "            segment_samples = int(self.sr * 5.0)\n",
    "            if len(y) > segment_samples:\n",
    "                max_start = len(y) - segment_samples\n",
    "                start = np.random.randint(0, max_start)\n",
    "                y = y[start:start + segment_samples]\n",
    "            else:\n",
    "                y = np.pad(y, (0, max(0, segment_samples - len(y))))\n",
    "\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y,\n",
    "            sr=self.sr,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=self.n_mels,\n",
    "            fmin=20,\n",
    "            fmax=16000\n",
    "        )\n",
    "        logmel = librosa.power_to_db(mel).astype(np.float32)\n",
    "        return logmel\n",
    "\n",
    "    def pad_or_crop(self, logmel):\n",
    "        _, t = logmel.shape\n",
    "        if t < self.target_len:\n",
    "            pad_width = self.target_len - t\n",
    "            logmel = np.pad(logmel, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            logmel = logmel[:, :self.target_len]\n",
    "        return logmel\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        audio_path = os.path.join(self.audio_root, row[\"filename\"])\n",
    "        \n",
    "        logmel = self.audio_to_logmelspec(audio_path)\n",
    "        logmel = self.pad_or_crop(logmel)\n",
    "        mel_tensor = torch.tensor(logmel, dtype=torch.float32).unsqueeze(0)  # [1, 128, T]\n",
    "\n",
    "        label_vec = torch.zeros(len(self.label_to_idx))\n",
    "        for label in row[\"parsed_labels\"]:\n",
    "            if label in self.label_to_idx:\n",
    "                label_vec[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return mel_tensor, label_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "948058be-1f5b-4ce9-9814-0480eb455970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(datatype=\"frequency_array\", data_path='/Users/rohitbogulla/Desktop/Sem 2/ML/Project/BirdClef/birdclef-2025/train_audio/'):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    if os.path.isdir(data_path):\n",
    "        train_dirs = os.listdir(data_path)\n",
    "    else:\n",
    "        train_dirs = []\n",
    "    parsed_labels = []\n",
    "\n",
    "    num_samples = 0\n",
    "\n",
    "    if datatype == \"frequency_array\":\n",
    "        for index in tqdm(train_dirs):\n",
    "            if index == \".DS_Store\":\n",
    "                continue\n",
    "            index_path = data_path+'/'+index\n",
    "            train_files = os.listdir(data_path+'/'+index)\n",
    "            for file in train_files:\n",
    "                file_path = index_path+'/'+file\n",
    "                X_train.append(ogg_to_frequency_array(file_path, 1024, 256))\n",
    "                num_samples+=1\n",
    "                # y_train.append(index)\n",
    "\n",
    "    if datatype == \"embedding\":\n",
    "\n",
    "        for index in tqdm(train_dirs):\n",
    "            if index == \".DS_Store\":\n",
    "                continue\n",
    "            index_path = data_path+'/'+index\n",
    "            train_files = os.listdir(data_path+'/'+index)\n",
    "            for file in train_files:\n",
    "                file_path = index_path+'/'+file\n",
    "                X_train.append(extract_yamnet_embedding(file_path))\n",
    "                num_samples+=1\n",
    "                # y_train.append(index)\n",
    "\n",
    "    if datatype == \"melspec\":\n",
    "        # for index in tqdm(train_dirs):\n",
    "        #     if index == \".DS_Store\":\n",
    "        #         continue\n",
    "        #     index_path = data_path+'/'+index\n",
    "        #     train_files = os.listdir(data_path+'/'+index)\n",
    "        #     for file in train_files:\n",
    "        #         file_path = index_path+'/'+file\n",
    "        #         X_train.append(audio_to_logmelspec(file_path, True))\n",
    "        #         parsed_labels.append(index)\n",
    "        #         num_samples+=1\n",
    "        train_data = pd.read_csv(data_path)\n",
    "        train_data[\"parsed_labels\"] = train_data.apply(\n",
    "                                        lambda row: list(set([row[\"primary_label\"]])),\n",
    "                                        axis=1)\n",
    "        num_classes = len(label_to_idx)\n",
    "        num_samples = len(train_data)\n",
    "        y = torch.zeros((num_samples, num_classes))\n",
    "    \n",
    "        for i, label_list in enumerate(train_data[\"parsed_labels\"]):\n",
    "            for label in label_list:\n",
    "                if label in label_to_idx:\n",
    "                    y[i, label_to_idx[label]] = 1\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=22)\n",
    "        train_idx, val_idx = next(mskf.split(train_data, y))\n",
    "        train_df = train_data.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = train_data.iloc[val_idx].reset_index(drop=True)\n",
    "        train_dataset = BirdClefDataset(\n",
    "                            df=train_df,\n",
    "                            label_to_idx=label_to_idx,\n",
    "                            audio_root=\"/Users/rohitbogulla/Desktop/Sem 2/ML/Project/BirdClef/birdclef-2025/train_audio\",\n",
    "                            random_crop=True\n",
    "                        )\n",
    "        val_dataset = BirdClefDataset(\n",
    "                            df=val_df,\n",
    "                            label_to_idx=label_to_idx,\n",
    "                            audio_root=\"/Users/rohitbogulla/Desktop/Sem 2/ML/Project/BirdClef/birdclef-2025/train_audio\",\n",
    "                            random_crop=True\n",
    "                        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,  num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    if datatype == \"melspec2\":\n",
    "        for index in tqdm(train_dirs):\n",
    "            if index == \".DS_Store\":\n",
    "                continue\n",
    "            index_path = data_path+'/'+index\n",
    "            train_files = os.listdir(data_path+'/'+index)\n",
    "            for file in train_files:\n",
    "                file_path = index_path+'/'+file\n",
    "                X_train.append(audio_to_logmelspec(file_path, True).flatten())\n",
    "                y_train.append(index)\n",
    "                num_samples+=1\n",
    "\n",
    "    \n",
    "    # num_classes = len(label_to_idx)\n",
    "    # y_train = np.zeros((num_samples, num_classes))\n",
    "\n",
    "    # # print(parsed_labels)\n",
    "\n",
    "    # for i, label in enumerate(parsed_labels):\n",
    "    #     # for label in label_list:\n",
    "    #     if label in label_to_idx:\n",
    "    #         y_train[i, label_to_idx[label]] = 1\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85f01928-1373-4f67-b562-4702124fc78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_training_data('melspec2','/Users/rohitbogulla/Desktop/Sem 2/ML/Project/BirdClef/birdclef-2025/train_audio_ff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0319401-ee63-4be8-8aba-df46a4cae73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339705c-279d-4f20-be6b-42f895355a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00a573-62a4-40ce-b359-bb38b7c66fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
